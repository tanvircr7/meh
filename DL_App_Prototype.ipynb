{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMR7eI3OWDJboaoKOoi/InH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvircr7/meh/blob/master/DL_App_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkjPCAJR0qbd",
        "outputId": "774b8cc5-2a7a-423b-af3c-5559beadd3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segment 1:\n",
            "\n",
            "Artificial intelligence has revolutionized many fields. From healthcare to transportation,\n",
            "AI systems are making our lives easier. Machine learning algorithms can now recognize patterns\n",
            "that humans might miss.\n",
            "<class 'str'>\n",
            "\n",
            "Segment 2:\n",
            "This has led to breakthrough discoveries in medicine. The ethics of AI is another important consideration. We must ensure AI systems are fair\n",
            "and unbiased. Privacy concerns have also emerged as AI systems collect more data.\n",
            "<class 'str'>\n",
            "\n",
            "Segment 3:\n",
            "These challenges require careful thought and regulation. Looking to the future, AI will continue to evolve. New architectures and algorithms\n",
            "are being developed.\n",
            "<class 'str'>\n",
            "\n",
            "Segment 4:\n",
            "The potential applications seem limitless. However, we must\n",
            "proceed with caution and wisdom.\n",
            "<class 'str'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "def segment_script(text, min_segment_size=3, parse_specificity=0.09):\n",
        "    \"\"\"\n",
        "    Segments a script into coherent sections using topic similarity\n",
        "\n",
        "    Args:\n",
        "        text (str): The input script text\n",
        "        min_segment_size (int): Minimum number of sentences per segment\n",
        "\n",
        "    Returns:\n",
        "        list: List of text segments\n",
        "    \"\"\"\n",
        "    # Download required NLTK data\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Create TF-IDF vectors for each sentence\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Calculate similarity between adjacent sentences\n",
        "    similarity_scores = []\n",
        "    for i in range(len(sentences)-1):\n",
        "        similarity = (tfidf_matrix[i] * tfidf_matrix[i+1].T).toarray()[0][0]\n",
        "        similarity_scores.append(similarity)\n",
        "\n",
        "    # Find segment boundaries where similarity is low\n",
        "    threshold = np.mean(similarity_scores) - parse_specificity*np.std(similarity_scores)\n",
        "    boundaries = [0]\n",
        "    current_size = 0\n",
        "\n",
        "    for i, score in enumerate(similarity_scores):\n",
        "        current_size += 1\n",
        "        if score < threshold and current_size >= min_segment_size:\n",
        "            boundaries.append(i + 1)\n",
        "            current_size = 0\n",
        "\n",
        "    boundaries.append(len(sentences))\n",
        "\n",
        "    # Create segments\n",
        "    segments = []\n",
        "    for i in range(len(boundaries)-1):\n",
        "        segment = ' '.join(sentences[boundaries[i]:boundaries[i+1]])\n",
        "        segments.append(segment)\n",
        "\n",
        "    return segments\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"\"\"\n",
        "Artificial intelligence has revolutionized many fields. From healthcare to transportation,\n",
        "AI systems are making our lives easier. Machine learning algorithms can now recognize patterns\n",
        "that humans might miss. This has led to breakthrough discoveries in medicine.\n",
        "\n",
        "The ethics of AI is another important consideration. We must ensure AI systems are fair\n",
        "and unbiased. Privacy concerns have also emerged as AI systems collect more data.\n",
        "These challenges require careful thought and regulation.\n",
        "\n",
        "Looking to the future, AI will continue to evolve. New architectures and algorithms\n",
        "are being developed. The potential applications seem limitless. However, we must\n",
        "proceed with caution and wisdom.\n",
        "\"\"\"\n",
        "\n",
        "segments = segment_script(sample_text)\n",
        "\n",
        "for i, segment in enumerate(segments):\n",
        "    print(f\"\\nSegment {i+1}:\")\n",
        "    print(segment)\n",
        "    print(type(segment))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(segments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rnbnKsT-SqT",
        "outputId": "5cf1f282-b813-4499-eccc-ec1cbb67fce7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "\n",
        "def extract_keywords(text, num_keywords=5):\n",
        "        \"\"\"\n",
        "        Extract key phrases from text segment for better prompt generation\n",
        "        \"\"\"\n",
        "        # Tokenize and remove stopwords\n",
        "        # lemmatizer = WordNetLemmatizer()\n",
        "        # lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
        "        # text = lemmatized_words\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [t for t in tokens if t.isalnum() and t not in stop_words]\n",
        "\n",
        "        # Get word frequencies\n",
        "        freq_dist = nltk.FreqDist(tokens)\n",
        "\n",
        "        # Get most common words\n",
        "        keywords = [word for word, freq in freq_dist.most_common(num_keywords)]\n",
        "\n",
        "        return keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEAq7v54cpod",
        "outputId": "a6220144-fb9d-408a-cac4-1838c5ff9dd0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for segment in segments:\n",
        "  print(extract_keywords(text=segment))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZattRcAac2PG",
        "outputId": "c58b45c0-d4aa-429a-dc14-999f327090ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'revolutionized', 'many', 'fields']\n",
            "['ai', 'systems', 'led', 'breakthrough', 'discoveries']\n",
            "['challenges', 'require', 'careful', 'thought', 'regulation']\n",
            "['potential', 'applications', 'seem', 'limitless', 'however']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-9OoMnqxczOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2NQorHHw-mJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}